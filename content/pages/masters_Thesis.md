Title: Master's Thesis
Date: December 11, 2023
Category: Paper
Summary: Submitted paper for Towson University Applied Information Technology Case Studies Course
Author: Shayda Banihashemi
Tags: Cloud, CI/CD, MongoDB, Docker, Terraform, Python, Git
Status: published

# Cloud and CI/CD Challenges

### I.	INTRODUCTION

The introduction of the public cloud created numerous new information technology solutions and capabilities. The advanced method of deploying these solutions is known as the continuous integration / continuous delivery (CI/CD) pipeline and is used to streamline the process of implementing hardware and software solutions in the public cloud. The CI/CD pipeline is a framework that includes different stages to perform the continuous integration and continuous delivery tasks. Each stage operates by using different tools to create the aggregate pipeline. Continuous integration includes the ongoing coding, building and testing tasks. Continuous delivery is the process of releasing new changes and deploying to production. Cloud solutions that are implemented using the CI/CD pipeline are gaining more traction throughout the Information Technology (IT) community around the world. It’s a solution with unique options and various benefits. It does, however, include many complexities in its implementation, utilization, and ongoing maintenance. There are many challenges to consider in deciding if this solution is the right one to implement. Organizations need to research and answer a series of questions before deciding whether to proceed with this complicated framework. Those are questions are as follows:
1.	What are the reasons an organization would want to deploy cloud solutions using CI/CD?

2.	Who are the various cloud providers and what services do they offer?

3.	What are the CI/CD stages and the corresponding products that should be researched and tested?

4.	How does the public cloud and the CI/CD pipeline impact IT administrators?

5.	What are the security challenges of running IT operations in the public cloud using the CI/CD pipeline?

6.	What best practices, policies and compliance requirements need to be included?

7.	When should the public cloud and CI/CD pipeline be used?

8.	What can be learned from other organizations that made this change?

Following these deliberations, a strategy should be developed to keep the organization focused on its endeavor. All of these considerations will help contribute to the success of the organization’s IT tasks and goals. Throughout this research, each of these issues will be analyzed to determine whether the complexities associated with deploying solutions in the public cloud through the CI/CD pipeline are justified or if they are too complex to be successfully implemented.  


### II.	CONVENTIONAL IT OPERATIONS 

Understanding how traditional IT teams operate within organizations as it relates to existing solutions, processes, and procedures can illustrate why so many organizations are moving to the cloud. Many organizations operate by having separate teams manage engineering and operational tasks based on various technologies. For example, server teams provide physical or virtual capabilities with various operating systems like Unix or Windows. The network team offers a variety of solutions like switching, routing and firewall capabilities based on internal and external customer needs, as well as load balancing options and more. The storage team provides solutions like Storage Area Network (SAN), Network Attached Storage (NAS) and Object Unstructured Data Storage for different workloads and their corresponding performance needs. The security team provides services like Identity Access Management (IAM), as well as vulnerability scanning and patching. Development teams are also a huge cornerstone of IT departments that can build out company specific applications without purchasing Commercial-off-the-Shelf (COTS) software solutions. These are just a few of the teams that most robust IT departments are likely to maintain. Each of these teams work with their management and business partners to understand their business needs. Based on that information, they begin to formulate plans and strategies to build and implement solutions. 
Infrastructure engineering teams often start by researching products offered by several different vendors and compare and contrast the capabilities advertised. They then make a selection from one of those options and run that system through a multitude of tasks in a test environment. This would include building out test cases for hardware systems or software programs based on the requirements that were previously gathered. Typically, functional tests would be done to make sure the required features operate as they are advertised. In the event that they are not working as they should, or that the team runs into a bug, feedback is provided to vendors so improvements and enhancements can be made. Engineers also need to run a series of performance tests that validate the vendor claims on various speed related metrics like Input Output Operations (IOPs), throughput, latency, and central processing unit (CPU) utilization. Those are followed up by high availability tests to check the resiliency and scalability of the system to make sure it can meet the demands of the workload. Once these respective engineering teams complete these tests, they need to make a decision to move forward with an investment or look for another product that would be a better fit and start the process again. 
At this stage, it’s common for conversations between the infrastructure, vendor, and vendor management teams to take place regarding the cost of the products and services, including support and maintenance. The next step would be to begin the installation in pre-production and production environments by making a series of requests to other teams. A data center location needs to be provided, IP addresses and DNS names need to be reserved, and port assignments and configurations need to be made. The engineers need to create a detailed cabling spreadsheet for onsite resources to follow in order to accurately run cables. This is a particularly important step that could have major consequences when events like code upgrades are done or maintenance is performed. If power and switching are not drawn to two independent and redundant sides, the system will not be highly available resulting in an extended outage that can have impacts on productivity, revenue, as well as the company brand. The next step of the implementation process is to configure the systems to prepare them for the workloads that will be migrated onto them. These tasks require the collaboration of resources across multiple teams, as well as project managers to make sure that tasks are completed on schedule in order to meet project deadlines. This process can become very resource intensive. 
After the initial engineering implementation steps are complete, operations teams take over support of day-to-day tasks like provisioning requests, monitoring the health of the systems, responding to alerts, maintaining security, managing code upgrades and analyzing capacity. The operations resources often spend a good amount of time with developers understanding their requirements for changes to existing applications, as well as the new applications that need to be deployed in the environment. The operations engineers need to understand the workload the applications will generate to right size the infrastructure needed to support the applications and provision the solution within a reasonable amount of time. If the latter cannot be done in an appropriate amount of time, the operations team risks becoming a bottleneck for the organization - stunting potential growth. 
A developer’s tasks usually starts with the steps outlined in the Software Development Life Cycle (SDLC). They meet with their business partners to understand their application requirements and begin their project work with the design phase. In a pre-procured dedicated sandbox environment, they begin developing their code and creating test cases. Team members collaborate with one another throughout the development process to craft the best solution possible. Several levels of user acceptance testing (UAT) with various participants are completed to test the code for functionality. They then work with the operations team to deploy the first version of code into production. Feedback is provided by testers and business partners so that enhancements to the code and application can be made to offer additional capabilities to end users. This process can take weeks to months to complete. These are the typical teams, roles and responsibilities that make up major IT departments found in the largest companies in the world. While this may be how many of them operate now, it is debatable if this is the best way to run and function in the future.

### III. CI/CD & CLOUD JUSTIFICATION

IT has always been in a state of change. Organizations have had to learn to adapt to changes and the teams that manage the solutions have also had to do the same.  When the right technology and methodologies are put to use, new products and services can be delivered to the market on an expedited schedule. If companies don’t take the opportunity to innovate their solutions, processes, and procedures, they often end up running their IT departments in an unproductive manner. Employees end up spending excessive amounts of time working on repeatable tasks that can and should be automated. These organizations are also slow to embrace changes that can put them in direct contact with the most cutting-edge IT solutions like those that are available in the public cloud. Companies that avoid looking towards the future often get stuck in the here and now restricting their ability to grow stronger and more profitable businesses. 
Operating the IT environment in this way keeps companies on legacy systems that are difficult to maintain because of the reduced vendor support. Staffing becomes a problem too since there are less resources to manage these aging systems (Mitchell). In order for individual resources and IT departments to avoid becoming stagnant in their roles and offerings, they must be capable of change. If they fail to do so, they will start to see their profits drop, they will most likely lose a percentage of the market share and in extreme situations, the organization could go out of business. With the incredible number of products and services offered to IT professionals in the market, employees must challenge the status quo within their teams in order to bring real change and value to the organizations that they support. Implementing solutions in the public cloud through the CI/CD pipeline allows organizations to do things bigger, better and faster. It helps organizations stay current with emerging technology. There are plenty of solutions and options to choose from in the cloud as well as the CI/CD pipeline which can be configured into custom solutions to be used by any organization. The solutions offer many different opportunities to automate, expedite deployments and scale solutions to provide enhanced and new products and services. This helps keep the company relevant and profitable. 

#### I. THE PUBLIC CLOUD

Twenty years ago, the notion of the public cloud was practically non-existent. CI/CD was not a commonly known acronym and the idea of Development Security Operations (DevSecOps)  hadn’t been introduced yet. These functions, however, are part of mainstream IT departments now. They are useful tools to implement new capabilities that increase the IT footprint and bring increased value to companies. Before the public cloud came to the market, engineering and operations teams followed drawn out processes to run their environments. There was limited space in data centers which restricted the number of solutions that could be tested. Engineers and project managers were constantly worried about missing procurement, implementation and migration deadlines. They were also always on the lookout for cost savings and generally concerned that they either had too much or too little capacity to rely on. 
The public cloud had a substantial impact on companies that embraced the opportunities that were available. Cloud providers like Amazon Web Service (AWS), Google Cloud Platform (GCP) and Microsoft Azure invest billions of dollars developing new offerings that are available worldwide. Many of these solutions haven’t been available through more conventional IT providers. Because of these investments, these companies are able to offer a plethora of services for server, storage, network and security needs with customizable features. AWS created the S3 storage solution that is classified as object storage. They offer a similar solution known as Glacier that also provides object stores, however, it’s classified as cold storage and isn’t meant to be accessed frequently and is more affordable. These storage systems manage data in an unstructured format that organizes artifacts in buckets. One of the most important features offered by the cloud providers with regard to their storage solutions are automated snapshots and backups for restores and disaster recovery events. Database solutions are available in a variety of choices that include traditional relational databases, in-memory options that provide caching features for low response times and document based or non-relational databases (“AWS Cloud Databases”). AWS server services offer EC2 virtual machines (VM) instances with auto-scaling features as well as Lightsail VM’s  that can be used for standing up and running websites (“Compute for Any Workload”). GCP offers similar solutions on their platform as well. Their network products include Cloud CDN that provides high performance and low latency response times that also has built-in features to protect operations from denial-of-service attacks (DOS) (“Leverage Google’s Decade of Experience Delivering Content”). Cloud DNS is a domain naming system (DNS) services that includes scaling and high availability features (“Cloud DNS”). Since security is a main concern of IT operations, core services like secret manager are available to store Application Programming Interface (API) tokens, keys and passwords (“Security and Identity"). Identity and Access Management services are included to accurately manage systems and permissions that users and groups are able to access through configurable roles (“Security and Identity"). Each of these providers offers many more cutting-edge technology solutions. These services help keep DevSecOps engineers focused on projects that can distinguish the organization’s products and services from their competitors, rather than being tied down by traditional IT tasks. 
The options provide teams with the opportunity to use agile methodologies and test different hardware and software solutions for their different workloads. Cloud solutions are offered worldwide to make high availability and scalability simpler. This means engineers don’t have to manage and facilitate complicated installations in foreign countries. They also don’t have to waste money over provisioning resources to make sure they don’t run out of capacity. Additionally, they don’t have to risk under provisioning which can lead to outages during traffic spikes. Events like this can have brand, profit and productivity impacts. Instead, they can build out automated procedures that pull from the capacity in the public cloud. Services can be configured to auto-scale based on current needs. 
Because there are different companies that offer services in the public cloud, IT teams can research the different products and services, costs, support models and availability of each, either through all the information provided on their website, through sales teams or by physically testing the options they are curious about. They can compare the results of each company to one another and make an informed decision as to which technologies/solutions in which they want to invest.
Companies no longer need to make huge upfront investments in technology which can make budgeting difficult and usually takes away from things like new investments and training. The introduction of the cloud gave Chief Financial Officers (CFOs) a new and interesting theory to contemplate - capital expenditure versus operational expenditure. Cloud providers charge their customers for what they use. This is considerably different from the traditional IT model where capacity requirements had to be guessed well in advance of actual needs. IT departments would need to purchase massive quantities of hardware and software capabilities and pay vendors for them up front; these types of assets depreciate in value over several years. 	With the cloud model, capital expenditure is replaced by operational expenditure (“Why More CFOS are Shifting IT Investment from Capex to Opex”). The cloud lets companies consume IT resources as they need them. They pay for services as they use them, which makes the impossible task of capacity planning a little easier. The cloud charges customers for what they use on a monthly basis. That doesn’t mean that organizations don’t need to remain vigilant about the amount of money they spend. There are usually heavy fees for pulling data back from the cloud to on premise systems known as egress fees. Companies also need to make sure that systems aren’t being provisioned and left unused and idle in the environment (“The Essential Need to Understand Cloud Costs in 2023”). They may not be in use but are still incurring costs. The cloud gives companies flexibility with how they can create and manage their budgets; however, they must remain disciplined in order to be ultimately successful.  

#### II. THE CI/CD PIPELINE

In the last couple of years, the term automation has gained a lot of popularity in the IT industry. Since IT is usually the team that is responsible for rolling out new products and services to the market, and everything has to be done bigger, better, and faster, automation has become a necessity. Before automation was such a big concept in IT, things had to be done in a slow monotonous manner. All tasks including testing, installation, provisioning and maintenance work had to be done manually. Running an IT department like this doesn’t give teams time to think about the future and be more innovative. It has them stuck in the past.
The CI/CD pipeline is a big help to IT teams from multiple different angles. It automates jobs and workloads, facilitates the delivery of new capabilities, and has the ability to increase system resources and efficiency. To understand how the CI/CD pipeline achieves all these goals, it’s important to understand what technology is included. Code repositories like Github, Bitbucket and Gitlab are used to keep the process of developing code organized. They let developers check in and check out their code, create branches off the main repository to avoid making changes to and breaking production code. Repositories keep an auditable history of each revision that was made. This allows developers to roll back their code in case a bug or an error is encountered. Developers can pull down different code repositories to work on and because it is a centralized location it makes collaboration between developers simple since the code is viewable in the repository once it is committed. Repositories can also kick off a series of automated actions through yaml  files that create containers, execute scripts, standup databases and provision infrastructure. While repositories can initiate automated actions, these steps can also be achieved by using dedicated orchestration applications as well. 
Containers are an important part of the CI/CD pipeline. One of the most recognizable brand names for containers is Docker and it is used by IT teams across the world. They are included in the CI/CD pipeline for a couple of key reasons. They stand up independent workspaces where everything that’s needed to run an application exists inside the container. That includes the code packages, libraries and dependencies that are requirements of the application. The underlying physical hardware has no impact on what runs inside the container and the resources the container uses are usually small in scope and have very little impact on the hardware. Containers are even capable of running multiple versions of the programming language in use so that developers can run the application using each version for even more robust testing. Volume mappings can be added to the container to save any data locally that is generated by the application. Port mappings can be included so that the container can make a connection to another container that may be running a database for example. Containers are lightweight and can easily be built and torn down to test code revisions and to help developers throughout the software development lifecycle. Container images can be created by developers to include environment-specific requirements needed by the application to run. Each time a container is built based on that image, everything that is needed is automatically installed. Even small features such as this help teams be more efficient and keep the environment more stable by avoiding inconsistencies. The alternative would be to use an image from a registry and add on dependencies each time a container is built from that image. 
Databases are another important component of the CI/CD pipeline. In the past, applications would have to rely on relational databases. They are typically complex to install, very costly and inflexible in the way that data is managed and stored. Tables are used in relational databases; entities have assigned attributes. Each record in the table must have a value for each attribute and they cannot store unstructured data. Although they are not very flexible, they do offer more reliability. Non-relational databases are much more flexible, cost-effective and can be scaled out. They use collections to store data in documents. Each document does not need to have the same fields. The flexibility of non-relational databases like MongoDB, Couchbase and Cassandra allows applications to ingest unstructured data faster and store it in JSON format . This lets developers read and pull from the database easily. Non-relational databases can be easily created in the public cloud when they are deployed through the CI/CD pipeline making them an excellent fit. 
Testing the scripts created by developers is a major part of the CI/CD pipeline. Ideally, after developers meet with their business partners to understand the requirements of the project, they should begin their development process by writing test cases, rather than actual application code. There are different third-party applications available on the market like pytest  and cucumber io  that help developers create automated test cases. The test cases are run each time the code is modified. If the DevSecOps engineer doesn’t get the result he or she was looking for, it’s much easier to determine where the break down occurred. Automated testing is also an important efficiency tool for developers saving them time from running manual tests each time they make changes and run their code. If tests were run manually each time the code was changed, engineers would never be able to match the volume of tests that can be completed when they are automated. It’s often very clear to spot when testing has not been included as part of the development process. Applications are typically very buggy; they don’t work the way they were intended to run and result in too many errors. The lack of quality in the application development process can lead to the breakdown of the relationship between the developers and the business partners. 
In order to get from scripts stored within the software repository to deploying infrastructure on the public cloud, another phase needs to be added to the CI/CD pipeline. Tools like Terraform are responsible for provisioning the infrastructure in the cloud. A connection from the local environment is made to the public cloud through a secure token. Once that is configured, infrastructure like servers with specific network requirements and object storage buckets can be deployed in specific regions in the cloud by running a script.

Putting each of these technology pieces together to create the CI/CD pipeline that is used in conjunction with the public cloud creates a process for infrastructure engineering and operations teams and developers to work together to deploy cutting-edge hardware and software solutions on a much shorter timeline through a repeatable process. The drastic reduction in time to stand up infrastructure and deploy homegrown applications brings enhanced and new products and services to the market more efficiently and gives the organization a huge competitive edge to increase their customer base and revenue.  

### III. DEVSECOPS

It’s common for employees on compute, storage, network and security teams to only focus on the needs of their own environment and work in a siloed manner in traditional IT environments. This leads to very slow deployments of infrastructure and application development. That has a big impact on the products and services a company can offer as well as their profitability. Having separate infrastructure, development and security teams come together to create integrated solutions became the origin of the DevSecOps engineering role.
DevSecOps has had incredible impacts on the way the engineering, operations, security and development teams work together to implement their solutions. As a result, it can have even greater positive impacts on the company’s profitability. The cloud and CI/CD pipeline offer DevSecOps engineers more tools to work with that help implement more customized solutions. With the access to these different tools, DevSecOps engineers can right size their solutions with regard to the number of servers, containers, databases and network bandwidth needed to run their workload. This means that they avoid under or over provisioning their hardware and software needs. Over provisioning leads to wasted resources that are added expenses. This cuts into profits, although it doesn’t impact performance of the systems and applications. Under provisioning resources can have huge impacts when there are traffic spikes. Performance suffers in situations like this because there isn’t enough bandwidth to handle the traffic and it ends up reducing productivity and profitability. It ultimately ends up damaging the company’s brand which can certainly have long term consequences. Customers can lose confidence in organizations that have very public failures which can cause even more losses in revenue. That’s why it is so consequential to implement the CI/CD pipeline managed by DevSecOps engineers to evolve into a modern and efficient IT department. 
Understanding how DevSecOps engineers do their jobs is crucial to understanding the benefits they bring to an organization. They leverage the steps of the CI/CD pipeline to deploy customized agile solutions to the cloud. This helps them design the best technology solutions for the applications their business partners want to deploy. Because of the number of resources in the cloud, they can develop a process that maintains the correct levels of resources to run their applications at any given time. When new application requests are made to the DevSecOps teams, a review of the requirements takes place first. The engineers research the different options available in the cloud and run test workloads on different platforms to prove which one is the best fit. The development of the actual application itself is now completely revolutionized as well. The speed of the automation qualities of the CI/CD pipeline allows developers to create code, run automated tests, push to the code repository, and have all different types of infrastructure provisioned (servers, containers, databases, etc.) in minutes. DevSecOps engineers can promote changes to production, get feedback from testers and business partners and incorporate even more changes in the code. The changes could either be fixes to the behavior of the application or new feature requests. This process of incorporating changes quickly facilitated by the cloud and the CI/CD pipeline removes the development process as a bottleneck and brings new products and service to the market for the company. It streamlines the development process in a way that brings increased value to the organization by being a catalyst for change. The process also places a lot of importance on the testing aspects of development. Because of this, the number of test cases that can be created and run each time the code is revised is groundbreaking. It is far beyond anything that could be done manually. Being surprised by an unexpected behavior in production that needs to be corrected is rare. The integration of the infrastructure and development teams in the DevSecOps role gives engineers more control over the size of the workload that can run at any given time. With the resources in the cloud, the CI/CD pipeline can be used to auto-scale all the resources that are needed to run workloads. Configurations can be added to the pipeline to add more servers, containers, databases, storage and more when certain thresholds are met - like the number of client connections, CPU and memory usage and high latency. 
Cloud deployments through CI/CD help DevSecOps engineers manage tasks by breaking them down into smaller more manageable chunks. Each phase of the pipeline is important because each piece has its own objectives that need to be accomplished, so they can all work together to make the entire process a success. From coding the application and managing it in a repository, building automated test cases, designing yaml files to build and start containers, or designing infrastructure config files to stand up systems in the cloud with specific server, storage, container, database, network, and security requirements - they all become more manageable when they are broken down into smaller pieces. The process would be much more daunting if all these parts had to managed, designed and implemented as a whole solution. 
The security aspect of the DevSecOps role is a little different compared to development and operations tasks that are included in this methodology. The security tasks that must be included to ensure the confidentially, integrity and availability of the environment act more as a wrapper around the entire DevOps process. Security needs to be taken into consideration in all areas of the development and operations phases. This includes things like security scans for known threats, vulnerabilities inadvertently created in code, and scheduled periodic updates to operating systems, packages, and libraries. Security principals relating to user accounts and permissions, group permissions as well as service accounts are also included in the DevSecOps responsibilities. Unused user accounts need to be deactivated and deleted and excessive permissions should be avoided. Regular audits need to be performed on a consistent basis to supplement routine clean-up activities. Security groups should be created and used for specific technology areas and systems. The levels of access should also factor into the permissions that will be granted to the group, such as read only, read and write and root access. The number of users added to these groups should be kept to a bare minimum. Service accounts should have limited access as well, such as disabling interactive system logins (“Security Best Practices”). All these accounts should be authenticated through the chosen authentication provider implemented by the organization. Without this step, rogue local accounts can be created and used without being regulated. 

Adding the DevSecOps engineering role to an IT team’s pool of resources is important to maximize the benefits of the public cloud through the CI/CD pipeline. This opens the organization up to faster, more frequent and consistent hardware and software implementations. It helps the company remain up to date with current trends, grows its confidence and popularity in the market and increases its customer base and profitability. 

